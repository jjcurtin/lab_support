```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) 
 conflict_prefer("filter", "dplyr")
 conflict_prefer("select", "dplyr")
 conflict_prefer("spec", "yardstick")

library(here)  
```

Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
library(tidyverse)  
library(janitor) 
library(lubridate)
library(ggplot2)
library(kableExtra)

theme_set(theme_classic()) 
```

Set additional paths
```{r}
path_job <- here(path_jobs, name_job)
path_output <- here(path_job, "output/output")
path_results <-  here(path_job, "output/results")
path_error <-  here(path_job, "output/error")
```

Source function scripts from lab support
```{r  source_script, message=FALSE, warning=FALSE}
source(here("../lab_support/chtc/fun_chtc.R"))
source(here("../lab_support/print_kbl.R"))
```

### Notes
This script aggregates all CHTC results for the job `r name_job`. It runs checks for 
missing jobs, plots hyperparameters, and summarizes model performance across all folds.    
  

Inputs:  

Returned CHTC files   

- output.zip  
- results.zip    
- output.zip   

Jobs input file   

- jobs.csv   

Output:   

- results_aggregate.csv   

### Unzip and check jobs

unzip chtc folders
```{r}
unzip(zipfile = file.path(path_job, "output/output.zip"), exdir = file.path(path_job, "output"))
unzip(zipfile = file.path(path_job, "output/results.zip"), exdir = file.path(path_job, "output"))
unzip(zipfile = file.path(path_job, "output/error.zip"), exdir = file.path(path_job, "output"))
```

Read in jobs
```{r}
jobs <- vroom::vroom(here(path_job, "input/jobs.csv"), col_types = vroom::cols())
```



check all error files are blank (0 kb)    
Note: error files also contain warnings
```{r}
err_files <- map_df(list.files(path_error, full.names = TRUE), file.info)
tabyl(err_files$size)
```

Pull error messages and jobs if error files are not all blank
```{r}
if (nrow(subset(err_files, size > 0)) > 0) {
  err_paths <- err_files %>% 
    filter(size > 0) %>% 
    rownames_to_column("path") %>% 
    pull(path) 
  for (i in err_paths) {
    err_i <- read_file(i) %>% 
    enframe(value = "message", name = NULL) %>% 
    mutate(job_num = as.numeric(str_remove(str_split(str_split(i, "/")[[1]][11], "_")[[1]][2], ".err")),
    message = str_remove_all(message, "\\n")) %>% 
    relocate(job_num)
    errs <- if (i == err_paths[1]) {
      err_i
    } else rbind(errs, err_i)
  }
  
  # print error messages
  print_kbl(errs, align = "l")
}
```


Job rows with error messages
```{r}
if (nrow(subset(err_files, size > 0)) > 0) {
  # print jobs
  jobs %>% 
    inner_join(errs, by = "job_num") %>% 
    print_kbl(align = "l")
} else print("no errors or warnings")
```


check all output files are blank (0 kb)
```{r}
out_files <- map_df(list.files(path_output, full.names = TRUE), file.info)
tabyl(out_files$size)
```

### Aggregate all result CSVs

read in all result CSVs
```{r}
result_files <- list.files(path_results, full.names = TRUE)
results <- vroom::vroom(result_files, col_types = vroom::cols()) %>% 
  glimpse()
```


check for missing jobs
```{r}
missing_job_nums <- enframe(seq(1:nrow(jobs)), name = NULL, value = "job_num") %>% 
  filter(!job_num %in% results$job_num)
```

`r nrow(results)` results from `r nrow(jobs)` jobs.      

```{r}
if (nrow(missing_job_nums) > 0) {
  print(str_c("missing ", nrow(missing_job_nums), " of ", nrow(jobs),
              " jobs: ", str_c(as.character(missing_job_nums$job_num), collapse=", ")))
  } else print("No missing jobs")
```



### Average metrics across folds 

NOTE: glmnet algorithms will have 1 returned model for each penalty/lambda combination. 
This has already been averaged across folds on the whole dataset in `tune_model()`.   

Other models (Knn and random forest) may have 10 - 100 models per unique configuration.   

```{r}
results_aggregate <- results %>% 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample) %>% 
  summarize(across(c(accuracy, bal_accuracy, sens, spec, roc_auc, n_feats),
                   mean),
            n_jobs = n(), .groups = "drop") 
```


`r nrow(results_aggregate)` unique model configurations.    
   

```{r}
results_aggregate %>% 
  print_kbl(digits = 4)
```

### Pull Best performing model by feature set

Summary of feature sets
```{r}
results_aggregate %>% 
  group_by(feature_set) %>% 
  summarise(n_feats_avg = mean(n_feats),
            n_feats_min = min(n_feats),
            n_feats_max = max(n_feats))
```

Get metrics
```{r}
feature_sets <- unique(results_aggregate$feature_set) 

for (i in feature_sets) {
  
  results_i <- results_aggregate %>% 
    filter(feature_set == i)
  
  print(str_c("Best performing model for ", i, " feature set"))
  
  if (slice_max(results_i, bal_accuracy)$algorithm == "glmnet") {
  results_i %>% 
    slice_max(bal_accuracy) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else {
  results_i %>% 
    slice_max(bal_accuracy) %>% 
    glimpse()
}
  
}
```


### Plot hyperparameters

```{r}
algorithms <- unique(results_aggregate$algorithm) 
for (k in algorithms) {
  
  results_k <- results_aggregate %>% 
      filter(algorithm == k)
  
  for (i in feature_sets) {
  
    results_i <- results_k %>% 
      filter(feature_set == i)
    
    
     # glmnet
    if (nrow(subset(results_i, algorithm == "glmnet")) != 0){
  
      plot_title <- str_c("Plotting glmnet hyperparameters for ", i, " feature set")
  
  
      plot_i <- results_i %>%
        filter(algorithm == "glmnet") %>% 
        mutate(hp1 = factor(hp1, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = log(hp2), 
                         y = bal_accuracy, 
                         group = hp1, 
                         color = hp1)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "mixture (alpha)") +
          labs(title = plot_title, x = "penalty (lambda)", y = "balanced accuracy")
  
      print(plot_i)
    }


    # random forest
    if (nrow(subset(results_i, algorithm == "random_forest")) != 0) {
      
      plot_title <- str_c("Plotting RF hyperparameters for ", i, " feature set")
      
      plot_i <- results_i %>%
        filter(algorithm == "random_forest") %>% 
        mutate(hp2 = factor(hp2, ordered = TRUE),
              resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = hp1, 
                         y = bal_accuracy, 
                         group = hp2, 
                         color = hp2)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "min n") +
          labs(title = plot_title, x = "mtry", y = "balanced accuracy")
      
       print(plot_i)
    }  
  
    # knn
    if (nrow(subset(results_i, algorithm == "knn")) != 0) {
      
      plot_title <- str_c("Plotting knn hyperparameters for ", i, " feature set")
      
      plot_i <- results_i %>%
        filter(algorithm == "knn") %>%
        mutate(resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = hp1, 
                         y = bal_accuracy)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          labs(title = plot_title, x = "neighbors", y = "balanced accuracy")
      
        print(plot_i)
    }
  }
}
```


### Overall best model performance

Highest balanced accuracy is `r round(max(results_aggregate$bal_accuracy), 2)`
```{r}
if (slice_max(results_aggregate, bal_accuracy)$algorithm == "glmnet") {
  results_aggregate %>% 
    slice_max(bal_accuracy) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else {
  results_aggregate %>% 
    slice_max(bal_accuracy) %>% 
    glimpse()
  }
```

highest ROC AUC is `r round(max(results_aggregate$roc_auc), 2)`
```{r}
# pull best AUC model if different than best balanced accuracy model
if (slice_max(results_aggregate, bal_accuracy)$algorithm == "glmnet" & slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results_aggregate %>% 
    slice_max(roc_auc) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else if (slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results_aggregate %>% 
    slice_max(roc_auc) %>% 
    glimpse()
  }
```




### Refit best model

Replicates best CHTC model using decision metric specified in study's training controls    
```{r}
best_model <- slice_max(results_aggregate, get(perf_metric)) %>% 
  glimpse()
```

Replicate splits and recipe
```{r}
# read in data
d <- vroom::vroom(here(path_job, "input/data_trn.csv"), col_types = vroom::cols())

# create splits object ---------------
set.seed(102030)
splits <- if (str_split(str_remove(cv_type, "_x"), "_")[[1]][1] == "group") {
  make_splits(d = d, cv_type = cv_type, group = group)
} else { 
  make_splits(d = d, cv_type = cv_type)
}

# build recipe ----------------
rec <- build_recipe(d = d, job = best_model)
```

Fit model and get results
```{r warning = FALSE}
results <- tune_best_model(best_model = best_model, rec = rec, folds = splits, cv_type = cv_type)
```

Pull predictions
```{r}
# separate predictions from results ----------------
predictions <- results[[2]]

# add subids by row number in original data set 
predictions <- predictions %>% 
  left_join(d %>% 
            rowid_to_column() %>% 
            select(rowid, subid, dttm_label), by = c(".row" = "rowid"))
```

`r nrow(predictions)` predictions for `r nrow(tabyl(predictions$subid))` participants      


Check number of predictions is correct    

- Single repeat cross validation should have one prediction for each held out observation (i.e., should equal number of total observations)    

FIX: Not sure how to check yet for bootstrap and repeated cross validation
```{r}
if (str_split(cv_type, "_")[[1]][1] == "group" | str_split(cv_type, "_")[[1]][1] == "kfold") {
  if (nrow(predictions) == nrow(d)) {
    print(str_c("number of predictions (", nrow(predictions), ") = number of observations (", nrow(d), ")")) 
    } else if (nrow(predictions) > nrow(d)) { 
      print(str_c("WARNING: number of predictions (", nrow(predictions), ") > number of observations (", 
                  nrow(d), ")"))
      } else if (nrow(predictions) < nrow(d)) print(str_c("WARNING: number of predictions (", 
                                                    nrow(predictions), ") < number of observations (", nrow(d), ")"))
}
```


plot predictions by participant   
*Change x-axis to day on study instead of exact date?*    
Note: this will need some more work to generalize if x is not a date time object
```{r fig.height = 60}
predictions %>% 
  group_by(subid) %>% 
  ggplot(aes(x = dttm_label, y = .pred_yes, color = y)) +
  geom_point(size = .9) +
  facet_wrap(~ subid, scales = "free_x", ncol = 3) + 
  scale_color_manual(values = c("gray70", "red3")) +
  theme(legend.position = "none") +
  ylim(0, 1)
```

Pull out best model results
```{r}
best_model_results <- results[[1]] %>% 
  glimpse()
```

Compare to CHTC best model
```{r}
best_model %>% 
  select(-n_jobs) %>% 
  glimpse()
```


pull out model fits    
Unnest .extracts column to see feature estimates
```{r}
best_model_fits <- results[[3]]
```


### Write out aggregated model configurations

Save configuration summaries
```{r}
write_csv(results_aggregate, here(path_job, "output/results_aggregate.csv")) %>% 
  glimpse()
```

Save best model
```{r}
write_csv(best_model_results, here(path_job, "output/best_model_results.csv")) %>% 
  glimpse()
```

Save best_model fits
```{r}
saveRDS(best_model_fits, here(path_job, "output/best_model_fits.rds"))
```

Save predictions from best model
```{r}
write_csv(predictions, here(path_job, "output/best_model_preds.csv")) %>% 
  glimpse()
```

