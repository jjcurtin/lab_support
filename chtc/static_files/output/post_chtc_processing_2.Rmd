```

Packages for lab workflow 
```{r, packages_workflow, message=FALSE, warning=FALSE}
library(conflicted) 
 conflict_prefer("filter", "dplyr")
 conflict_prefer("select", "dplyr")

library(here)  
```

Packages for script
```{r, packages_script, message=FALSE, warning=FALSE}
library(tidyverse)  
library(janitor) 
library(lubridate)
library(ggplot2)
library(kableExtra)

theme_set(theme_classic()) 
```

Set additional paths
```{r}
path_job <- here(path_jobs, name_job)
path_output <- here(path_job, "output/output")
path_results <-  here(path_job, "output/results")
path_error <-  here(path_job, "output/error")
```

Source print_kbl function from lab support
```{r  source_script, message=FALSE, warning=FALSE}
source(here("../lab_support/print_kbl.R"))
```


### Notes
This script aggregates all CHTC results for the job `r name_job`. It runs checks for 
missing jobs, plots hyperparameters, and summarizes model performance across all folds.    
  

Inputs:  

Returned CHTC files   

- output.zip  
- results.zip    
- output.zip   

Jobs input file   

- jobs.csv   

Output:   

- results_aggregate.csv   

### Unzip and check jobs

unzip chtc folders
```{r}
unzip(zipfile = file.path(path_job, "output/output.zip"), exdir = file.path(path_job, "output"))
unzip(zipfile = file.path(path_job, "output/results.zip"), exdir = file.path(path_job, "output"))
unzip(zipfile = file.path(path_job, "output/error.zip"), exdir = file.path(path_job, "output"))
```

Read in jobs
```{r}
jobs <- vroom::vroom(here(path_job, "input/jobs.csv"), col_types = vroom::cols())
```



check all error files are blank (0 kb)    
Note: error files contain warnings
```{r}
err_files <- map_df(list.files(path_error, full.names = TRUE), file.info)
tabyl(err_files$size)
```

Pull error messages and jobs if error files are not all blank
```{r}
if (nrow(subset(err_files, size > 0)) > 0) {
  err_paths <- err_files %>% 
    filter(size > 0) %>% 
    rownames_to_column("path") %>% 
    pull(path) 
  for (i in err_paths) {
    err_i <- read_csv(i, col_types = readr::cols()) %>% 
      mutate(job_num = as.numeric(str_remove(str_split(str_split(i, "/")[[1]][10], "_")[[1]][2], ".err"))) %>% 
      relocate(job_num)
    errs <- if (i == err_paths[1]) {
      err_i
    } else rbind(errs, err_i)
  }
  
  # print error messages
  print_kbl(errs, align = "l")
}
```


```{r}
if (nrow(subset(err_files, size > 0)) > 0) {
  # print jobs
  jobs %>% 
    inner_join(errs, by = "job_num") %>% 
    print_kbl(align = "l")
}
```


check all output files are blank (0 kb)
```{r}
out_files <- map_df(list.files(path_output, full.names = TRUE), file.info)
tabyl(out_files$size)
```

### Aggregate all result CSVs

read in all result CSVs
```{r}
result_files <- list.files(path_results, full.names = TRUE)
results <- vroom::vroom(result_files, col_types = vroom::cols()) %>% 
  glimpse()
```


check for missing jobs
```{r}
missing_job_nums <- enframe(seq(1:nrow(jobs)), name = NULL, value = "job_num") %>% 
  filter(!job_num %in% results$job_num)
```

`r nrow(results)` results from `r nrow(jobs)` jobs.      

```{r}
if (nrow(missing_job_nums) > 0) {
  print(str_c("missing jobs: ", str_c(as.character(missing_job_nums$job_num), collapse=", ")))
  } else print(str_c("No missing jobs"))
```



### Average metrics across folds 

NOTE: glmnet algorithms will have 1 returned model for each penalty/lambda combination. 
This has already been averaged across folds on the whole dataset in `tune_model()`.   

Other models (Knn and random forest) may have 10 - 100 models per unique configuration.   

```{r}
results_aggregate <- results %>% 
  group_by(algorithm, feature_set, hp1, hp2, hp3, resample) %>% 
  summarize(across(c(accuracy, bal_accuracy, sens, spec, roc_auc),
                   mean),
            n_jobs = n(), .groups = "drop") 
```


`r nrow(results_aggregate)` unique model configurations.    
   

```{r}
results_aggregate %>% 
  print_kbl(digits = 4)
```

### Pull Best performing model by feature set

```{r}
feature_sets <- unique(results_aggregate$feature_set) 

for (i in feature_sets) {
  
  results_i <- results_aggregate %>% 
    filter(feature_set == i)
  
  print(str_c("Best performing model for ", i, " feature set"))
  
  if (slice_max(results_i, bal_accuracy)$algorithm == "glmnet") {
  results_i %>% 
    slice_max(bal_accuracy) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else {
  results_i %>% 
    slice_max(bal_accuracy) %>% 
    glimpse()
}
  
}
```


### Plot hyperparameters

```{r}
algorithms <- unique(results_aggregate$algorithm) 
for (k in algorithms) {
  
  results_k <- results_aggregate %>% 
      filter(algorithm == k)
  
  for (i in feature_sets) {
  
    results_i <- results_k %>% 
      filter(feature_set == i)
    
    
     # glmnet
    if (nrow(subset(results_i, algorithm == "glmnet")) != 0){
  
      plot_title <- str_c("Plotting glmnet hyperparameters for ", i, " feature set")
  
  
      plot_i <- results_i %>%
        filter(algorithm == "glmnet") %>% 
        mutate(hp1 = factor(hp1, ordered = TRUE),
               resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = log(hp2), 
                         y = bal_accuracy, 
                         group = hp1, 
                         color = hp1)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "mixture (alpha)") +
          labs(title = plot_title, x = "penalty (lambda)", y = "balanced accuracy")
  
      print(plot_i)
    }


    # random forest
    if (nrow(subset(results_i, algorithm == "random_forest")) != 0) {
      
      plot_title <- str_c("Plotting RF hyperparameters for ", i, " feature set")
      
      plot_i <- results_i %>%
        filter(algorithm == "random_forest") %>% 
        mutate(hp2 = factor(hp2, ordered = TRUE),
              resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = hp1, 
                         y = bal_accuracy, 
                         group = hp2, 
                         color = hp2)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          scale_color_discrete(name = "min n") +
          labs(title = plot_title, x = "mtry", y = "balanced accuracy")
      
       print(plot_i)
    }  
  
    # knn
    if (nrow(subset(results_i, algorithm == "knn")) != 0) {
      
      plot_title <- str_c("Plotting knn hyperparameters for ", i, " feature set")
      
      plot_i <- results_i %>%
        filter(algorithm == "knn") %>%
        mutate(resample = case_when(resample == "none" ~ "none_19",
                                    TRUE ~ resample)) %>% 
        separate(resample, c("resample", "under_ratio"), "_") %>% 
        mutate(under_ratio = factor(under_ratio, levels = c("1", "3", "19"))) %>% 
        ggplot(mapping = aes(x = hp1, 
                         y = bal_accuracy)) +
          geom_line() +
          facet_grid(under_ratio ~ resample) +
          labs(title = plot_title, x = "neighbors", y = "balanced accuracy")
      
        print(plot_i)
    }
  }
}
```


### Overall best model performance

Highest balanced accuracy is `r round(max(results_aggregate$bal_accuracy), 2)`
```{r}
if (slice_max(results_aggregate, bal_accuracy)$algorithm == "glmnet") {
  results_aggregate %>% 
    slice_max(bal_accuracy) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else {
  results_aggregate %>% 
    slice_max(bal_accuracy) %>% 
    glimpse()
  }
```

highest ROC AUC is `r round(max(results_aggregate$roc_auc), 2)`
```{r}
# pull best AUC model if different than best balanced accuracy model
if (slice_max(results_aggregate, bal_accuracy)$algorithm == "glmnet" & slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results_aggregate %>% 
    slice_max(roc_auc) %>% 
    mutate(hp2 = log(hp2)) %>% 
    glimpse()
} else if (slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results_aggregate %>% 
    slice_max(roc_auc) %>% 
    glimpse()
  }
```


```{r}
# Print fits for model with highest balanced accuracy (for repeated single models - not glmnet)
if (slice_max(results_aggregate, bal_accuracy)$algorithm != "glmnet") {
  (best_model_bal_accuracy <- results %>% 
    select(-job_num) %>%
    inner_join(results_aggregate %>% 
                 slice_max(bal_accuracy) %>% 
                 select(-c(accuracy:n_jobs)),
               by = c("algorithm", "feature_set", "hp1", "hp2", "hp3", "resample")) %>% 
    arrange(n_fold, n_repeat)) %>% 
    print_kbl(height = "100%", caption = "Fits for model with highest balanced accuracy") 
}
```


```{r}
# Fits for model with highest ROC AUC (if different configuration and not glmnet)
if (slice_max(results_aggregate, bal_accuracy)$algorithm != "glmnet" & slice_max(results_aggregate, bal_accuracy)$roc_auc != slice_max(results_aggregate, roc_auc)$roc_auc) {
  results %>% 
    select(-job_num) %>% 
    inner_join(results_aggregate %>% 
                 slice_max(roc_auc) %>% 
                 select(-c(accuracy:n_jobs)),
               by = c("algorithm", "feature_set", "hp1", "hp2", "hp3", "resample")) %>% 
    arrange(n_fold, n_repeat) %>% 
    print_kbl(height = "100%", caption = "Fits for model with highest roc")
}
```


### Plot/visualize model performance 

FIX: How to plot observed vs predicted outcomes with just performance metrics? 
Refit best model?    



### Write out aggregated model configurations

Save configuration summaries
```{r}
write_csv(results_aggregate, here(path_job, "output/results_aggregate.csv")) %>% 
  glimpse()
```


